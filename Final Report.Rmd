---
title: "Does Clickbait Lead to More Clicks?"
author: "Hsuanyi Cheng, Patrick Kim, Kayla Wopschall, Ziwei Zhao"
date: "8/12/2021"
output:
  pdf_document: default
  html_document: default
  word_document: default
header-includes: 
- \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

## Background

## Research Question

## Hypothesis
 

# Facebook Ads Experiment

## Design

We chose Facebook advertising as our preferred platform for the experiment for several reasons. First, it is one of the most widely used social media platforms, which would allow flexibility in choosing the sample population and ensure that we have enough subjects in the experiment given our limited budget. Secondly, Facebook has an intuitive A/B testing feature that allows us to easily design ad campaigns with a control ad and a treatment ad that never get shown to the same Facebook user. 

We conducted two pilot experiments. In each experiment, we use one existing news article and design two ads for it, each with a headline and an image. For the ad that gets shown to the control group, both the headline and image are rather neutral. They state facts without a call to action or question for the reader, and the image should not elicit strong emotional reactions. For the ad shown to the treatment group, both the headline and image are "clickbait" - the headline contains a question that entices the reader to click on the article to find the answer, and the image portray more dynamic action that might draw attention. 

Figure 1 shows the two ads used for the first pilot experiment. It uses the same NPR article about traveling overseas during the pandemic. The control group sees the headline "Traveling overseas - everything you need to know" with an image of a woman hiking in an idyllic nature scene. The treatment group sees the headline "Traveling overseas? It might not be worth it" with an image of travelers getting their temperatures checked in an airport security line. 

```{r, echo=FALSE, fig.cap="Facebook experiment example ads 1", out.width = '50%', fig.align = "center", fig.pos = "H"}
knitr::include_graphics("images/fb_travel.PNG")
```


## Analysis

```{r, message=FALSE, warning=FALSE, include = FALSE}
library(data.table)
library(sandwich)
library(lmtest)
library(stargazer)
library(ggplot2) 

robust_se <- function(mod, type = 'HC3') { 
  sqrt(diag(vcovHC(mod, type)))
  }
```

```{r load data, include = FALSE}
d <- fread('data/fb_data_detail.csv')
d[ , treated := adsetname == 'Treatment']
d[ , female := gender=='female']
d[ , mean(click), by = treated]
```

these are results xxxxxx

```{r simple model, results='asis', echo=FALSE}
model1 <- d[ , lm(click ~ treated)]
stargazer(model1 ,
          type = 'latex',
          se = list(robust_se(model1)),
          header = FALSE
          )
```

## Findings 

conclusions from the fb experiment

# Survey Experiment 

Because we spent so much time trying to do the Facebook experiment, we unfortunately had to redesign, deploy, and analyze a new experiment shortly before the deadline. We chose to utilize SurveyMonkey because their platform allows for surveying users cheaply and efficiently with little danger of being locked out of the platform. The respondent is shown two image ads and asked to indicate which ad they would be more likely to click on.

## Design

SurveyMonkey contains an A/B testing feature that allows for different text and images to be shown to respondents at random. We utilize this feature to show two different images to the respondent and create many sampling pairs. First, two ads with tame language and normal stock images were created by the team. For each ad, one version is created with extreme language in the title, another version with a shocking image, and a final version with both modified at once. This means that there are four versions of each article, and each version has a 25% chance of being displayed to the respondent according to SurveyMonkey's randomization procedure. 

#### For each article

1. Control
2. Text Treatment
3. Image Treatment
4. Both

Since there are two articles, this creates 16 sample pairings to analyze.

## Analysis 

analyze survey results 

### Within test

### Between test

# Conclusions 

Overall conclusions 

