---
title: "Does Clickbait Lead to More Clicks?"
author: "Hsuanyi Cheng, Patrick Kim, Kayla Wopschall, Ziwei Zhao"
date: "8/12/2021"
output:
  pdf_document: default
fontsize: 11.5pt
header-includes: 
- \usepackage{float}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Abstract 

We study the effect on click-through rates of applying textual and stylistic image often related to clickbait to headlines of newspaper articles which can be bought in a digital environment. Publishing a dataset consisting of original headline, rewritten headline and re-applying image in social media and survey platform, we can directly measure whether these “clickbait features” do what they are believed to do: entice readers to click on them. The main findings are as follows. First, the data shows that image or both text and image click-bait features lead to a statistically significant increase in the number of clicks. Second, re-write headline-only articles shows no statistically significant increase in the number of clicks. The findings are concluded by two different experiments through various platforms.

# Introduction

The way people consume newspaper articles is changing: more and more newspaper articles are consumed on the internet rather than from physical newspapers. People used to buy a newspaper, read it from cover to cover while scanning headlines, and reading articles that they thought were interesting (Holmqvist et al. 2003). However, increasingly more people are reading individual articles online, outside of their original publication. Often, a person reads this article because it was shared on social media or some other internet platform. According to Journalism portal, in a 2017 survey, they found  two-thirds of readers utilized social media to get fresh news. 

With this change, the function of the headline of a news article changed as well. Previously, the primary function of a headline and image were to give the reader, who was scanning the newspaper, a clear understanding of what the article was about. But in social media the headline and image become primary ways to attract the reader's attention and make the reader curious as to what the article is about, so that it lures the reader to open the article. Thus we started to wonder if attractive headlines and images, often called click-bait, did draw more attention than regular newspaper headlines.

Clickbait is a commonly explored device, with varied results (Mini, 2021). While clickbait, strictly defined, is the crafting of sensational headlines and images that lead to misleading content, we want to test the impact of the clickbait strategies within various other contexts(e.g. news, politics, popular science). As content developers, you’re constantly trying to drive engagement with readers, and structuring your meta content (headlines, subheadings, and images) on social media platforms should influence active engagement with your content, and the types of individuals you engage with. 

In the experiment, we design the control group to view a headline that is factual, unemotional with a similarly neutral but relevant image. Often captured in the newspaper's original writing. The treatment group will be a more emotional headline and/or image that is designed to trigger someone to click through due to curiosity, anxiety, or any other feelings pushed forward by the headline. Both groups would arrive at the same article if they click through on the ad. The mechanisms we’re trying to test here are people’s interest is piqued more by one type of image and/or rhetoric over another, enough to engage with the content (clicks, likes, comments). Through our various treatment groups we attempt to see the effects of images, headlines, and the combination of the two. 

We initially utilized Facebook ads platform to execute an A/B test, in which control and treatment image and headline will render to different users separately. In just a few days, we successfully collected more than thousand data points to analyze the click bait effects. Unfortunately, Facebook ads algorithms detected our experiments and considered it against their policy. Even though we tried to explain and provided personal identification to prove legit execution. In the end, we decided to run a survey via Survey Monkey to continue the same experiment with a different design approach. In the second stage of the experiment, we send out the survey displaying both control and treatment with different articles to avoid obvious click-bait tests. In both experiments, we are able to archive randomized and unbiased results by the platform setting. 
 

# Facebook Ads Experiment

## Design

We chose Facebook advertising as our preferred platform for the experiment for several reasons. First, it is one of the most widely used social media platforms, which would allow flexibility in choosing the sample population and ensure that we have enough subjects in the experiment given our limited budget. Secondly, Facebook has an intuitive A/B testing feature that allows us to easily design ad campaigns with a control ad and a treatment ad that never get shown to the same Facebook user. 

We conducted two pilot experiments. In each experiment, we use one existing news article and design two ads for it, each with a headline and an image. For the ad that gets shown to the control group, both the headline and image are rather neutral. They state facts without a call to action or question for the reader, and the image should not elicit strong emotional reactions. For the ad shown to the treatment group, both the headline and image are "clickbait" - the headline contains a question that entices the reader to click on the article to find the answer, and the image portray more dynamic action that might draw attention. 

Figure 1 shows the two ads used for the first pilot experiment. It uses the same NPR article about traveling overseas during the pandemic. The control group sees the headline "Traveling overseas - everything you need to know" with an image of a woman hiking in an idyllic nature scene. The treatment group sees the headline "Traveling overseas? It might not be worth it" with an image of travelers getting their temperatures checked in an airport security line. 

```{r, echo=FALSE, fig.cap="Facebook Pilot Experiment 1", out.width = '50%', fig.align = "center", fig.pos = "H"}
knitr::include_graphics("images/fb_travel.PNG")
```

Figure 2 shows the two ads used for the second pilot experiment. It uses an New York Times article about the drought on the West Coast. The control group sees the headline "What to understand about the drought in the west" with an image of a red fiery sky. The treatment group sees the headline "Will you run out of water?" with a farmer walking on barren land in front of a few cows.

```{r, echo=FALSE, fig.cap="Facebook Pilot Experiment 2", out.width = '50%', fig.align = "center", fig.pos = "H"}
knitr::include_graphics("images/fb_drought.PNG")
```

Each experiment got to run for about two days. Because the business page from which we created ads is brand new, and we did not have much, if any, previously advertising records on Facebook, our respective advertising accounts got disabled by Facebook. It was eye opening to see efforts being made by social media platforms to limit "fake news", even though it was a big roadblock for our experiment. However, we were able to extract data from these pilot studies and see some meaningful results.


## Analysis

### Regression Model 

The first regression analysis is linear regression model of the variable click on the variable treated - both indicator variables. Click indicates whether a Facebook user clicked through to the article after seeing the ad, and treated indicates if a user is in the treatment group. Table 1 shows the results. The first column shows the results from the first pilot study using the travel ads. The coefficient on treated is 0.122 with a 0.00 p-value, meaning that people in the treatment group are 12.2% more likely to click on the ad. The second column shows results from the second pilot study using the drought ads. The coefficient on treated is 0.020 with a p-value of 0.0691. People in the treatment group are 2.0% more likely to click on the ad, but it is only only statistically significant at the 10% level, compared to the 1% level in the first pilot study. 

```{r, message=FALSE, warning=FALSE, include = FALSE}
library(data.table)
library(sandwich)
library(lmtest)
library(stargazer)
library(ggplot2) 

robust_se <- function(mod, type = 'HC3') { 
  sqrt(diag(vcovHC(mod, type)))
  }
```

```{r load data, include = FALSE}
d_fb <- fread('data/fb_data_detail.csv')
d_fb[ , treated := adsetname == 'Treatment']
d_fb[ , female := gender=='female']
d_fb[ , mean(click), by = treated]

d2 <- fread('data/fb_data_detail_2.csv')
d2[ , treated := adsetname == 'Treatment']
d2[ , female := gender=='female']
d2[ , mean(click), by = treated]
```

```{r simple model, results='asis', echo=FALSE}
model1 <- d_fb[ , lm(click ~ treated)]
model2 <- d2[ , lm(click ~ treated)]
stargazer(model1 , model2, 
          type = 'latex',
          se = list(robust_se(model1), robust_se(model2)),
          header = FALSE,
          title = 'Facebook experiment - click regressed on treated',
          column.labels = c('travel ads', 'drought ads')
          )
cat('\\newpage') #For some reason, the stargazer model leaks to the next page without this fix

```
### Covariate Check 

Since Facebook's algorithms randomized our populations for the experiments and their business goals are presumed to be focused on revenue from advertisers, we conducted covariate checks to ensure that randomization was done properly. We conducted a regression of treated on female to see if gender predicted which group a user would fall in. The female coefficient is not statisticaly significant in the travel ads experiment, but it is significant at the 10% level in the drought ads experiment.

```{r fb test covariates gender, echo = FALSE, message = FALSE, warning = FALSE, results = 'asis', fig.pos = "H"}
test_gender <- d_fb[ , lm(treated ~ female)]
test_gender2 <- d2[ , lm(treated ~ female)]

stargazer(test_gender, test_gender2,
          type = 'latex',
          se = list(robust_se(test_gender),
                    robust_se(test_gender2)),
          title = 'Facebook experiment - covariate check - gender',
          column.labels = c('travel ads', 'drought ads'),
          header = FALSE
          )

```

We ran a similar regression of treated on age. Age was provided from facebook in buckets: 18-24, 25-34, 35-44, 45-54, 55-64 and 65+. The regression results effectively compare each age group to the 18-24 group. For both the travel ads experiment and the drought ads experiment, all coefficients on age groups are positive. None of the coefficients are statistically significant in the travel ads experiment, but they are all statistically significant at different levels for the drought experiment. Without other covariates or a deeper understanding of Facebook's algorithm, it is difficult for us to tell why these the drought experiment's randomization may not have been done properly.

```{r fb test covariates age, echo = FALSE, message = FALSE, warning = FALSE, results = 'asis', fig.pos = "H"}
test_age <- d_fb[ , lm(treated ~ as.factor(age))]
test_age2 <- d2[ , lm(treated ~ as.factor(age))]

stargazer(test_age, test_age2,
          type = 'latex',
          se = list(robust_se(test_age),
                    robust_se(test_age2)),
          title = 'Facebook experiment - covariate check - age',
          column.labels = c('travel ads', 'drought ads'),
          header = FALSE
          )

cat('\\newpage')
```


\newpage


# Survey Experiment 

Because we spent so much time trying to do the Facebook experiment, we unfortunately had to redesign, deploy, and analyze a new experiment shortly before the deadline. We chose to utilize SurveyMonkey because their platform allows for surveying users cheaply and efficiently with little danger of being locked out of the platform. The respondent is shown two image ads and asked to indicate which ad they would be more likely to click on. The survey targeted 400 respondents from across the United States using the broadest demographic available on SurveyMonkey. The full dataset contains 437 responses as SurveyMonkey provided these extra responses for free.

## Design

SurveyMonkey contains an A/B testing feature that allows for different text and images to be shown to respondents at random. We utilize this feature to show two different images to the respondent and create many sampling pairs. First, two ads with tame language and normal stock images were created by the team. For each ad, one version is created with extreme language in the title, another version with a shocking image, and a final version with both modified at once. This means that there are four versions of each article, and each version has a 25% chance of being displayed to the respondent according to SurveyMonkey's randomization procedure. 

#### For each ad:

1. Control
2. Text Treatment
3. Image Treatment
4. Both


```{r, echo=FALSE, fig.cap="Article 1", out.width = '85%', fig.align = "center", fig.pos = "H"}
knitr::include_graphics("images/article_1.png")
```


```{r, echo=FALSE, fig.cap="Article 2", out.width = '85%', fig.align = "center", fig.pos = "H"}
knitr::include_graphics("images/article_2.png")
```

Since there are two ads and four possible versions of each ad, this creates 16 possible pairings, meaning that each pairing has a 1 in 16 chance of appearing in a respondent's survey.


```{r, echo=FALSE, fig.cap="Survey Sample", out.width = '85%', fig.align = "center", fig.pos = "H"}
knitr::include_graphics("images/survey_sample.png")
```

## Analysis 

``` {r survey analysis, results = 'asis', message = FALSE, warning = FALSE, error = FALSE, echo = FALSE}
library(dplyr)
library(ggplot2)
library(tidyr)
library(sandwich)
library(stargazer)
library(lmtest)

d <- read.csv(file = 'data/News Pulse.csv')
d <- as.data.frame(d)

#Data cleaning
d <- d[-1,]
df <- d %>% select(c("Respondent.ID", "What.type.of.news.are.you.most.interested.in.", 
                    "Article.1", "Article.2", "Which.article.would.you.be.more.likely.to.click.on.",
                    "How.interested.are.you.in.the.news.on.a.scale.of.1.to.10.", "Gender",
                    "Household.Income", "Device.Type", "Region", "Age"))
colnames(df) <- c("id", "news_interest", "article1", "article2", "article_pref", 
                 "news_interest_score", "gender", "household_income", "device_type", 
                 "region", "age")

#Replaces article column values with something more manageable
#Change article preference to a binary variable
article_helper <- function(line) {
  if(line == "Text Treatment") {"T"}
  else if(line == "Image Treatment") {"I"}
  else if(line == "Text and Image Treatment") {"B"}
  else {"C"}
}


df <- df %>% mutate(article1 = sapply(article1, article_helper, USE.NAMES=FALSE),
                  article2 = sapply(article2, article_helper, USE.NAMES=FALSE),
                  article_pref = ifelse(article_pref == "Article 1", 0, 1))

#Helper function for prop testing
prop_test_helper <- function(group, data = df) {
  filtered_data <- data %>% filter(article1 == group & article2 == group)
  prop.test(sum(filtered_data$article_pref), length(filtered_data$article_pref))
}

#Test of proportions for each treatment group pair
prop_test_p_values <- c()
for (g in c("C", "I", "T", "B")) {
  prop_test_p_values[g] <- prop_test_helper(g)$p.value
}

```

Respondents only ever see one version of each ad, which means that the potential outcomes of a respondent observing a treated ad and a control ad are never observed. To overcome this problem, a test of proportions is used as a basic check to determine whether the difference in ads has a significant effect on respondent choices. For each treatment type, we compare the number of times that ad version was selected by the respondents between both ads, and the p-values obtained are outlined below.

#### Test of Proportion P-values:

* Control group: `r prop_test_p_values["C"]`
* Text treatment group: `r prop_test_p_values["T"]`
* Image treatment group: `r prop_test_p_values["I"]`
* Both treatment group: `r prop_test_p_values["B"]`

Evidence of a difference in outcomes based on the ad is weak according to our dataset.

## Within test

## Between test

# Conclusions 

In the Facebook ads platform experiment, the results suggest that with ‘clickbait’ images and text get significantly more clicks. In the survey results, we analyzed as “within subjects” and “between subjects”. In the Within subjects analysis, it suggests that both Image and text modifications have a positive ATE, and lead to the most preferences for ‘clickbait’ type articles. Image and Text modifications both have significant positive ATE’s, with Text being slightly more impactful than image.

In the Between subjects analysis, it suggests that when Article 1 is Control, there is a positive significant ATE from Image Treatment and a negative significant ATE from Both treatments combined. But when Article 2 is controlled, there is no significant ATE from Article 1 Treatments.

Overall, we believe that sensational headlines and images did attract people’s attention and the image has a more dramatic effect than the text. Our experiment's result can only reveal partial evidence due to unexpected platform difficulties. To extend this experiment and analysis, we could navigate different platforms and various content types.



